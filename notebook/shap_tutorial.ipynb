{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39862958d23443c0",
   "metadata": {},
   "source": [
    "# SHAP UTILIZATION TUTORIAL – Regression Decision Tree Model\n",
    "## Explainable AI for Climate–Health Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d70607f90b3e5d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This tutorial demonstrates how to use SHAP (SHapley Additive exPlanations) to interpret a regression model built for climate–health predictions.\n",
    "The workflow follows the CHAP-core structure and explains each step of the SHAP process in a clear and practical manner.\n",
    "\n",
    "### By the end of this tutorial, you will understand:\n",
    "\n",
    "- What SHAP values represent\n",
    "\n",
    "- How to compute SHAP values for a trained model\n",
    "\n",
    "- How to generate global and local explanations\n",
    "\n",
    "- How to visualize and interpret climate variable contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb521f64f15d66",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies\n",
    "\n",
    "In this step, we import all required Python libraries used for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dd1d3b270935c07",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb24e0b77f1313d5",
   "metadata": {},
   "source": [
    "## 3. Load the Trained Model\n",
    "\n",
    "We use a DecisionTreeRegressor that was previously trained on climate–health data and saved in the model/ directory."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"../output/brazil_model.bin\"\n",
    "\n",
    "print(\"Loading model:\", model_path)\n",
    "model = joblib.load(model_path)"
   ],
   "id": "414ae68dd14fa25a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Why this matters:**\n",
    "\n",
    "SHAP explains the behavior of an existing model. We don't train here; we only interpret."
   ],
   "id": "8a9257096ef40602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Load the Dataset (CHAP-Compatible)\n",
    "\n",
    "We load the historic dataset used for training.\n",
    "Typical CHAP-style fields include:\n",
    "\n",
    "- rainfall\n",
    "\n",
    "- mean_temperature\n",
    "\n",
    "- disease_cases\n",
    "\n",
    "- time_period\n",
    "\n",
    "- location"
   ],
   "id": "73c059e883f84726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = \"../data/historic_brazil.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna()\n",
    "\n",
    "features = [\"rainfall\", \"mean_temperature\"]\n",
    "target = \"disease_cases\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X.head()"
   ],
   "id": "2c4cafca96ceed79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***Why this matters:***\n",
    "\n",
    "SHAP needs the same feature matrix the model was trained on."
   ],
   "id": "ffe57fb285d249a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Understanding SHAP (Short Explanation)\n",
    "\n",
    "SHAP values answer:\n",
    "\n",
    "*“How much did each feature contribute to the model’s prediction for this instance?”*\n",
    "\n",
    "***Key ideas:***\n",
    "\n",
    "- SHAP is based on cooperative game theory\n",
    "\n",
    "- A positive SHAP value means the feature increases the prediction\n",
    "\n",
    "- A negative SHAP value means the feature decreases the prediction\n",
    "\n",
    "- The magnitude shows how strong the impact is\n",
    "\n",
    "- SHAP works well with tree-based models with its corresponding methode"
   ],
   "id": "ecfba414899b5b13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Initialize the SHAP Explainer\n",
    "\n",
    "Because our model is a DecisionTreeRegressor, we use TreeExplainer."
   ],
   "id": "2b4e647d31363c73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)"
   ],
   "id": "efd414b25ef0397f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What happens here:**\n",
    "\n",
    "SHAP computes a contribution score for every feature in every row of the dataset."
   ],
   "id": "af8d6a45f9f80dfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Global Explanations\n",
    "\n",
    "Global explanations help answer:\n",
    "\n",
    "***“Which features are most important overall?”***\n",
    "\n",
    "#### 7.1 SHAP Summary Plot\n",
    "\n",
    "This plot shows:\n",
    "\n",
    "- Feature importance\n",
    "\n",
    "- Direction of influence\n",
    "\n",
    "- Distribution of SHAP values"
   ],
   "id": "7055f82f75ea0843"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "175c4fa373972875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 7.2 SHAP Bar Plot (Mean Absolute Contribution)\n",
    "\n",
    "This shows the average impact magnitude of each feature."
   ],
   "id": "e13169091e4b4995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d23e201d18715553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Local Explanations\n",
    "\n",
    "A local explanation interprets one prediction in detail.\n",
    "\n",
    "Pick an example index:"
   ],
   "id": "7e38d9eee23e27da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index = 100\n",
    "sample = X.iloc[index:index+1]\n",
    "sample"
   ],
   "id": "b8772aeb84e1dbc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Compute SHAP values for this sample**",
   "id": "aebd96ed9424cc96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_shap = explainer.shap_values(sample)",
   "id": "2fbe5e5037f22143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 8.1 Force Plot\n",
    "\n",
    "A force plot shows how each feature pushes the prediction up or down relative to the baseline."
   ],
   "id": "9aac14711c1f8a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    sample_shap,\n",
    "    sample\n",
    ")"
   ],
   "id": "c14472609052c522"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Dependence Plots\n",
    "\n",
    "Dependence plots show how one feature affects predictions across the dataset.\n",
    "\n",
    "**Rainfall effect**"
   ],
   "id": "bc49a5002355597c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "shap.dependence_plot(\"rainfall\", shap_values, X)",
   "id": "dca0f9e8bb512f41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Interpretation:**\n",
    "\n",
    "These plots help understand nonlinear patterns—for example, how vector-borne disease risk increases within specific rainfall and temperature ranges."
   ],
   "id": "b9a659a2621ca145"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Summary & Learning Outcomes\n",
    "\n",
    "In this tutorial, we:\n",
    "\n",
    "- Loaded a trained regression decision tree model\n",
    "\n",
    "- Imported climate–health data\n",
    "\n",
    "- Initialized a SHAP explainer\n",
    "\n",
    "- Computed SHAP values\n",
    "\n",
    "- Produced global explanation plots\n",
    "\n",
    "- Generated local explanation plots\n",
    "\n",
    "- Interpreted how rainfall and temperature drive disease predictions\n",
    "\n",
    "- Followed a CHAP-style folder structure for outputs\n",
    "\n",
    "## Explainability Pipeline"
   ],
   "id": "b45612ad34fed6c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
